{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression using `keras`\n",
    "\n",
    "As the [Keras Documentation](http://keras.io/) states, \"Keras is a minimalist, highly modular neural networks library, written in Python and capable of running on top of either TensorFlow or Theano\". While it's not really meant to be used to perform multiple linear regression, we'll walk through it using `keras` to continue with the theme of building up from linear regression to neural networks. \n",
    "\n",
    "## Computational Graphs for Multiple Linear Regression \n",
    "\n",
    "As a reference, the computational graphs that we used to visualize the forward and backward propagation steps in solving our multiple linear regression problem with gradient descent are as follows: \n",
    "### Forward Propagation\n",
    "\n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/custom/mult_linear_comp_graph_condensed_forprop.png?raw=true\" width=300\\>\n",
    "\n",
    "### Backward Propagation\n",
    "\n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/custom/mult_linear_comp_graph_condensed_backprop.png?raw=true\" width=400\\>\n",
    "\n",
    "### Performing Multiple Linear Regression with Keras\n",
    "\n",
    "Since `keras` can be run on top of either `tensorflow` or `theano`, this means that under the hood of our multiple linear regression using `keras`, a similar version of the code that we wrote in our `theano` or `tensorflow` implementation is being run. By default, `keras` runs on `theano`, but by [adjusting our keras configuration file](http://keras.io/backend/#switching-from-one-backend-to-another), we can easily change that. For now, though, we'll just run it on `theano`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Input\n",
    "from keras.layers.core import Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from datasets.general import gen_multiple_linear\n",
    "from utils.plotting import plot_errors\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_keras_model(): \n",
    "    learning_rate = 0.1\n",
    "    # 1. Specify a placeholder for the inputs. \n",
    "    xs = Input(shape=(4,))\n",
    "    # 2. Define the equation that generates predictions. \n",
    "    ys = Dense(1, activation='linear', bias=False)(xs)\n",
    "\n",
    "    # 3. Define a `Model` object that will be used to train/learn the coefficients. \n",
    "    linear_model = Model(input=xs, output=ys)\n",
    "    \n",
    "    # 4. Define the optimizer and loss function used to train/learn the coefficients. \n",
    "    sgd = SGD(learning_rate)\n",
    "    \n",
    "    # 5. Compile the model (basically, build up the backpropagation steps)\n",
    "    linear_model.compile(loss='mean_squared_error', optimizer=sgd)\n",
    "    \n",
    "    return linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `get_keras_model` function returns back what our `get_theano_graph` ([notebook 2c](https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/02-multiple-linear/2c_nn_th.ipynb)) and `get_tensorflow_graph` ([notebook 2d](https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/02-multiple-linear/2d_nn_tf.ipynb)) functions returned - a set of computations that perform forward and backward propagation in order to solve a multiple linear regression problem using gradient descent. \n",
    "\n",
    "Compared to prior implementations, our `get_keras_model` has a smaller code base, which makes sense given it's goal to be a \"minimalist, highly modular neural networks library\". We see that our forward propagation is defined in 2 steps, compared to the 5 steps it took with `theano` or `tensorflow`: \n",
    "\n",
    "* Step `1` is simply the `keras` way of generating a placeholder variable that will later be replaced with real data. The one piece of information we have to provide is the dimensionality of one of our input observations (e.g. how many features it has). Since we specified that we would have three features and added a column of ones into our `xs` to account for the intercept (\n",
    "<img src=\"../imgs/variables/beta0.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=20 \\>\n",
    "), our `shape` needs to have 4 dimensions.  \n",
    "* Step `2` defines our multiple linear regression equation, <img src=\"../imgs/equations/mult_linear_3_feats.png\" width=125 style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" \\>. We'll detail the `Dense` class at a later point when we cover some of the terminology of neural networks. For now, just trust that it defines our multiple linear regression equation. \n",
    "\n",
    "Backward propagation is defined in steps `4-5`: \n",
    "\n",
    "* Step `4` specifies exactly how to perform our gradient descent updates. Here, we'll use what we've used in all of our prior implementations - gradient descent with a learning rate of 0.1. As we'll see in later notebooks, there are a number of more complicated flavors of gradient descent that we also have the option to use.\n",
    "* Step `5` tells `keras` to calculate the update rules for our coefficients, defining each of the steps necessary for doing so. Here, we have to specify a `loss` as well as an `optimizer`. As discussed above, the `optimizer` specifies how to perform our gradient descent updates. The `loss` function specifies how we calculate the error, which up to this point has been using squared error. After defining both a `loss` and `optimizer`, `keras` has all of the pieces it needs to calculate the update rules for our coefficients, and to add those update steps into the graph that it will later run through. \n",
    "\n",
    "Step `3` builds a model object that we can later use to learn our coefficients. To instantiate it, we have to specify `input` as well as `output`. In order to finish building it for later use, we have to run `compile` on it like we did in step `5`. \n",
    "\n",
    "Let's now use our `keras` model to actually solve a multiple linear regression problem..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Error: 1.278908923207922e-10\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEZCAYAAABSN8jfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAHipJREFUeJzt3X+4HVV97/H3J4n51YQECwSbQPgVpQa9ERGi4OUoIiFU\n4nOtLcEriK1Nb4mgV70gtiXap7VYReFSi1G0BpSoWDTViIErB38HBEJCTMiJSPgVQiMESPgVku/9\nY+bgZOecfebsObNnn70/r+fZz5lZs2bmu9dzcr5Za2bWKCIwMzMrYkTVAZiZ2fDnZGJmZoU5mZiZ\nWWFOJmZmVpiTiZmZFeZkYmZmhTmZmLUhScslvbvqOKxzOJlYS5J0n6SnJT0p6an05+VVx5Ul6T2S\nVkvaIelhSZ+XNKkJ5z0h0ybbJe2uaadpETE3Iq4uOxazXvJDi9aKJP0WeG9E3Jyj7siI2DVQ2WCP\nMUD9DwEfBs4CfgRMBf4N2B94Q0S8kPdYRWKTNB24FxgV/sdsFXLPxFqZ+iyUzpb0U0mXStoKXNxP\nmST9bdrLeUTSv0vaJz3G9PR/9O+VtAn4f5LGSLpG0lZJj0taKWn/Ps4/EVgELIyIGyNiV0TcD/wZ\ncAjwPyW9LO1ZTc7s9xpJ/yVpZLr+Xkm/lvQ7ST+QdHCm7m5JfyNpA7BhsG0l6WZJ7+2jvR6XtFHS\n69Py+9O2OSuz72hJn5a0SdLmtMc1JkcM1sGcTGy4Og7YCBwA/GM/ZeeQ9BxOBA4DJgJX1BznvwOv\nAE4Bzk7rTAVeCvw18Ewf534DMAa4PlsYETuA5cDJEbEZ+DnwjkyV+cC3ImKXpHnAhcDbSXozPwGu\nrTnPPOB1wCvrtkQ+xwKrSL7XtcBS4BjgcODdwBWSxqd1LwGOAF6d/pwK/P0QxGBtzMnEWtl3JD2W\n/m/6MUl/kdn2UER8PiJ2R8Rz/ZSdCVwaEZsi4mngo8AZknp/7wO4OCKeTevvBP4QeHkk7oyI7X3E\ntR+wNSJ297Ftc7odkj/aZ2a2nQF8LV1eAHwyIjakx/lnYJakgzL1/ykinsh8vyJ+GxFL0qGwbwDT\ngI9HxM6IuBF4niRxALwP+GB67h1pbPOHIAZrY6OqDsCsjnl1rpk8kKPsj4BNmfVNJL/zUzJlD2aW\nl5D8kV2aXki/BvhYH9crtgL7SRrRR0J5Wbod4NvA5ZKmAEcCuyLiZ+m26cBlkj6TroskuU3NfI9s\nbEVtySw/AxARW2vKJqTDeuOB26UXR85G0M+Qo1kv90ysldX7A9bXxebasodJ/mj3mk7S+8j+YX1x\nn/Taxz9ExEySoay3kQyT1foF8BzwP/YIVpoAnArclB5vG7CCpEcyn2Roqdf9wIKIeGn62TciJkTE\nLwf4jmXbCjwNzMzENjkiSr9LzYY3JxNrZ9cCH5R0SPqH/h+BpZneRO1F6y5JR6XDYNtJEs9eQ1kR\n8STwCeD/SjpF0ihJh5AMH91P0qPJxnAWybWTr2fKvwBcJOmV6bknSfrTBr9nI72GPvdJh8G+CHyu\n9+YDSVMlvbXB2KxDOJlYK/vP9LmJ3s+3B7n/l4GrgR8DvyH5H/d5me21//M/ELgOeAJYC9yc7r+X\niPgX4CLg02n9X5AMo70lInZmqi4DZgCbI2JNZv/vkFyLWCppG7AamFMntnry9NIG2p5dv5DkRoZf\nprGtAF4+iHisA5X+nImkOcDnSBLXVRFxSc32VwBfAY4GLoqIS/Pua2ZmraHUZJIOF2wATiIZv74N\nOCMi1mfq7Ecylv124PHeZJJnXzMzaw1lD3MdC/Skt2buJLkAOS9bISK2RsTtQO0TwwPua2ZmraHs\nZJK9zRGSWx2nNmFfMzNrIl+ANzOzwsp+aPEh4ODM+rS0bEj3leQJ7szMBikihuxh1LJ7JrcBR6ST\n6o0meXhrWZ362S82qH0joqM+F198ceUxtNLH7eE2cXsM7jPUSu2ZRDKh3UKS+9R7b+9dJ2lBsjkW\np1NN/Ipkgr3dks4HXhkR2/vat8x4zcysMaXPzRURN5DMypot+0JmeQtwUO1+/e1rZmatxxfgh6mu\nrq6qQ2gpbo+9uU325PYoV1u8aVFStMP3MDNrFknEMLoAb2ZmHcDJxMzMCnMyMTOzwpxMzMysMCcT\nMzMrzMnEzMwKczIxM7PCnEzMzKwwJxMzMyvMycTMzApzMjEzs8KcTMzMrDAnEzMzK8zJxMzMCnMy\nMTOzwpxMzMysMCcTMzMrzMnEzMwKczIxM7PCnEzMzKwwJxMzMyvMycTMzApzMjEzs8KcTMzMrDAn\nEzMzK8zJxMzMCnMyMTOzwpxMzMysMCcTMzMrzMnEzMwKczIxM7PCnEzMzKwwJxMzMyvMycTMzAor\nPZlImiNpvaQNki7op87lknokrZI0K1P+QUl3S1ot6WuSRpcdr5mZDV6pyUTSCOAK4BRgJjBf0pE1\ndU4FDo+IGcAC4Mq0/I+A9wNHR8SrgVHAGWXGa2ZmjSm7Z3Is0BMRmyJiJ7AUmFdTZx6wBCAiVgKT\nJE1Jt40E/kDSKGA88HDJ8ZqZWQPKTiZTgQcy6w+mZfXqPARMjYiHgc8A96dl2yLiphJjNTOzBo2q\nOoD+SJpM0muZDjwBXCfpzIj4el/1Fy1a9OJyV1cXXV1dTYjSzGx46O7upru7u7TjKyLKO7g0G1gU\nEXPS9QuBiIhLMnWuBG6OiG+k6+uBE4E3AqdExPvS8ncDx0XEwj7OE2V+DzOzdiOJiNBQHa/sYa7b\ngCMkTU/vxDoDWFZTZxlwFryYfLZFxBaS4a3ZksZKEnASsK7keM3MrAGlDnNFxC5JC4EVJInrqohY\nJ2lBsjkWR8RySXMlbQR2AOek+94q6TrgTmBn+nNxmfGamVljSh3mahYPc5mZDc5wG+YyM7MO4GRi\nZmaFOZmYmVlhTiZmZlaYk4mZmRXmZGJmZoU5mZiZWWFOJmZmVpiTiZmZFeZkYmZmhTmZmJlZYU4m\nZmZWmJOJmZkV5mRiZmaF1U0mkkZKurlZwZiZ2fBUN5lExC5gt6RJTYrHzMyGoTxvWtwOrJF0I8mb\nEAGIiPNKi8rMzIaVPMnkP9KPmZlZn3K9tlfSaODl6eo9EbGz1KgGya/tNTMbnKF+be+APRNJXcBX\ngfsAAQdJOjsifjxUQZiZ2fA2YM9E0u3AmRFxT7r+cuDaiHhtE+LLxT0TM7PBGeqeSZ7nTF7Sm0gA\nImID8JKhCsDMzIa/PBfgfyXpS8A16fq7gF+VF5KZmQ03eYa5xgDnAiekRT8BPh8Rz5UcW24e5jIz\nG5yhHuaqm0wkjQSWRMS7huqEZXAyMTMbnKZeM0mfgJ+e3hpsZmbWpzzXTO4FfiZpGXs+AX9paVE1\nYPduGOFpK83MKpEnmfwm/YwAJpYbTuNeeAFGu/9kZlaJuskkvWYyMSI+3KR4GuZkYmZWnTzXTI5v\nUiyF7GypCV7MzDpLnmGuVen1km+x5zWTlpr88YUXqo7AzKxz5UkmY4HfAW/OlAUtNpOwk4mZWXUG\nTCYRcU4zAinKw1xmZtXp95qJpG9mli+p2baizKAa4Z6JmVl16l2An5FZPrlm2/4lxFKIk4mZWXXq\nJZN685PknrtE0hxJ6yVtkHRBP3Uul9QjaZWkWZnySZK+JWmdpLWSjuvvPB7mMjOrTr1rJuMlvYYk\n4YxLl5V+xuU5uKQRwBXAScDDwG2SvhsR6zN1TgUOj4gZabK4Epidbr4MWB4R75Q0Chjf37ncMzEz\nq069ZLIZ6J0y5ZHMcu96HscCPRGxCUDSUmAesD5TZx6wBCAiVqa9kSnAM8AbI+I96bYXgCf7O5GT\niZlZdfpNJhHxpiE4/lTggcz6gyQJpl6dh9KyXcBWSV8B/hvJO1TOj4hn+jqRh7nMzKrTylMjjgKO\nBv41Io4GngYu7K+yeyZmZtXJ89BiEQ8BB2fWp6VltXUO6qfOAxHR+1bH64A+L+ADfPGLi1iR3rDc\n1dVFV1dXw0GbmbWb7u5uuru7Szv+gG9aLHTwZKLIe0guwG8GbgXmR8S6TJ25wLkRcZqk2cDnImJ2\nuu0W4H0RsUHSxcD4iNgroUiKm24KTjqptK9iZtZWhvrlWP32TCQdXW/HiLhjoINHxC5JC4EVJENq\nV0XEOkkLks2xOCKWS5oraSPJ3F/ZJ+7PA74m6SUk71Xp92l8D3OZmVWn356JpJvTxbHAMcBdJLcF\nvxr4VUS8vikR5iApvve94LTTqo7EzGx4aNpreyPiTekdXZuBoyPimIh4LfAa9r7uUTnfzWVmVp08\nd3O9IiLW9K5ExN3AH5cXUmM8zGVmVp08d3OtlvQl4Jp0/V3A6vJCaoyTiZlZdfIkk3OA/wWcn67/\nGPi30iJqkIe5zMyqk+d9Js9KupJkjqx7mhBTQ9wzMTOrzoDXTCSdDqwCbkjXZ6Wv8W0pTiZmZtXJ\ncwH+YpL5tLYBRMQq4NAyg2qEh7nMzKqTJ5nsjIgnasrKe2y+Qe6ZmJlVJ88F+LWSzgRGSppB8lT6\nz8sNa/CcTMzMqpOnZ/J+YCbwHPB14AngA2UG1QgPc5mZVaduzySdqPETEfFh4GPNCakx7pmYmVWn\nbs8kInYBJzQplkKcTMzMqpPnmsmd6a3A3yKZ1ReAiPiP0qJqgIe5zMyqkyeZjAV+B7w5UxZASyUT\n90zMzKqT5wn4ft8h0kqcTMzMqjNgMpE0FvgLkju6xvaWR8R7S4xr0DzMZWZWnTy3Bl8NHAicAtxC\n8o72p8oMqhFOJmZm1cmTTI6IiL8DdkTEV4HTgOPKDWvwnn++6gjMzDpXrulU0p/bJB0FTAIOKC+k\nxjiZmJlVJ8/dXIsl7Qv8HbAMmAD8falRNcDJxMysOnnu5vpSungLcFi54TTOycTMrDp57ubqsxcS\nEZ8Y+nAa52RiZladPMNcOzLLY4E/AdaVE07jnEzMzKqTZ5jrM9l1SZ8GflhaRA1yMjEzq06eu7lq\njSd51qSlOJmYmVUnzzWTNfz+zYojgf2BlrpeAk4mZmZVynPN5E8yyy8AWyKi5WbCeu65qiMwM+tc\neZJJ7dQp+0h6cSUiHhvSiBrknomZWXXyJJM7gIOAxwEBk4H7021Bizx74mRiZladPBfgbwTeFhH7\nRcQfkgx7rYiIQyOiJRIJOJmYmVUpTzKZHRHLe1ci4gfAG8oLqTFOJmZm1ckzzPWwpL8FrknX3wU8\nXF5IjXEyMTOrTp6eyXyS24GvTz8HpGUtxcnEzKw6ioiBa/VWTmYP3haD2akJJMWIEcGuXVVHYmY2\nPEgiIjRwzXz67ZlI+ntJR6bLYyT9CNgIbJH0lqEKYCj5PfBmZtWoN8z158A96fLZad0DgBOBfyo5\nrkEbPdpDXWZmVamXTJ7PDGedAlwbEbsiYh35LtwDIGmOpPWSNki6oJ86l0vqkbRK0qyabSMk3SFp\nWb3zOJmYmVWnXjJ5TtJRkvYH3gSsyGwbn+fgkkYAV5Ako5nA/N6hs0ydU4HDI2IGsAC4suYw5wO/\nHuhcY8Y4mZiZVaVeMjkfuA5YD3w2In4LIGkucGfO4x8L9ETEpojYCSwF5tXUmQcsAYiIlcAkSVPS\nc00D5gJfYgDumZiZVaff4ar0D/uRfZQvB5bvvUefpgIPZNYfJEkw9eo8lJZtAT4LfASYNNCJnEzM\nzKrTyPtMmkLSaSQzFK8imROs7i1sTiZmZtXJfSG9QQ8BB2fWp6VltXUO6qPOnwKnp8Nq44CJkpZE\nxFl9nejxxxdx2WUwZQp0dXXR1dU1VN/BzGzY6+7upru7u7TjD+qhxUEfXBpJcnvxScBm4FZgfnpH\nWG+ducC5EXGapNnA5yJids1xTgQ+FBGn93OeeO1rgyuvhGOOKevbmJm1j6F+aDFXz0TSG4BDsvUj\nYslA+0XELkkLSe4EGwFcFRHrJC1INsfiiFguaa6kjcAO4JwGvgejR/sFWWZmVcnz2t6rgcOBVUDv\nhCVBegfWQCLiBuAVNWVfqFlfOMAxbgFuqVfH10zMzKqTp2dyDPDKVpuPq5aTiZlZdfLczXU3cGDZ\ngRTlhxbNzKqTp2eyH/BrSbcCL16V6O9ieFV8zcTMrDp5ksmisoMYCmPHwrPPVh2FmVlnGjCZpBe/\nW56TiZlZdQa8ZiJptqTbJG2X9LykXZKebEZwgzFuHDzzTNVRmJl1pjwX4K8geU1vD8mT6H8J/GuZ\nQTVi3Dj3TMzMqpJrbq6I2AiMTN9n8hVgTrlhDd7Yse6ZmJlVJc8F+KcljQZWSfoUybQoLTdB5Lhx\n8NRTVUdhZtaZ8iSFd6f1FpJMd3IQ8I4yg2qEL8CbmVUnz91cmySNA14WER9vQkwN8QV4M7Pq5Lmb\n620k83LdkK7PGuh97FVwMjEzq06eYa5FJG9H3AaQvqzq0BJjaoiHuczMqpMnmeyMiCdqylpu0kf3\nTMzMqpPnbq61ks4ERkqaAZwH/LzcsAbPPRMzs+rk6Zm8H5hJMsnjtcCTwAfKDKoR7pmYmVUnz91c\nTwMfSz8ty8nEzKw6/SaTge7YarUp6D3MZWZWnXo9k9cDD5AMba0EhuzF82Vwz8TMrDr1ksmBwMkk\nkzyeCXwfuDYi1jYjsMHyRI9mZtXp9wJ8OqnjDRFxNjAb2Ah0S1rYtOgGwRM9mplVp+4FeEljgNNI\neieHAJcD15cf1uB5mMvMrDqK6Pv5Q0lLgKOA5cDSiLi7mYENhqTYtSsYNQp27QK19NUdM7PqSSIi\nhuyvZb1ksptklmDY84l3ARER+wxVEEVJiohgzBh44olkyMvMzPo31Mmk32GuiGi5d5YMpHeoy8nE\nzKy5hl3CqGf8eHj66aqjMDPrPG2VTCZMgO3bq47CzKzzOJmYmVlhTiZmZlaYk4mZmRXWdslkx46B\n65mZ2dBqu2TinomZWfM5mZiZWWFOJmZmVpiTiZmZFeZkYmZmhZWeTCTNkbRe0gZJF/RT53JJPZJW\nSZqVlk2T9CNJayWtkXTeQOdyMjEzq0apyUTSCOAK4BRgJjBf0pE1dU4FDo+IGcAC4Mp00wvA/46I\nmSSvED63dt9aTiZmZtUou2dyLNATEZsiYiewFJhXU2cesAQgIlYCkyRNiYhHImJVWr4dWAdMrXcy\nJxMzs2qUnUymAg9k1h9k74RQW+eh2jqSDgFmASvrnczJxMysGnVf29sKJE0ArgPOT3sofVq0aBGb\nN0NPD3R3d9HV1dW0GM3MWl13dzfd3d2lHb/fNy0OycGl2cCiiJiTrl9I8pbGSzJ1rgRujohvpOvr\ngRMjYoukUcD3gB9ExGV1zhMRQU8PzJkDv/lNaV/JzKwtDPWbFsse5roNOELSdEmjgTOAZTV1lgFn\nwYvJZ1tEbEm3fRn4db1EkrXvvrBt29AEbmZm+ZU6zBURuyQtBFaQJK6rImKdpAXJ5lgcEcslzZW0\nkeSd8+8BkHQ88C5gjaQ7Sd5Df1FE3NDf+SZPTt4Bv3s3jGirJ2jMzFpbqcNczdI7zAUwcSI8+CBM\nmlRxUGZmLWy4DXM1nYe6zMyary2TyeOPVx2FmVlncTIxM7PC2i6ZTJ7sYS4zs2Zru2TinomZWfM5\nmZiZWWFtl0w8zGVm1nxtl0xe+lJ47LGqozAz6yxtl0z23x8efbTqKMzMOkvbJZMpU2DLloHrmZnZ\n0HEyMTOzwtoumRx4oJOJmVmztV0ymTwZnnkGnn226kjMzDpH2yUTCQ44wBfhzcyaqe2SCfi6iZlZ\nszmZmJlZYW2ZTKZNS16QZWZmzdGWyeSQQ+C++6qOwsysc7RlMpk+3cnEzKyZ2jKZuGdiZtZcbZtM\nNm2qOgozs86hiKg6hsIkRfZ77N4N48cn7zUZN67CwMzMWpQkIkJDdby27JmMGAGHHQY9PVVHYmbW\nGdoymQDMnAlr11YdhZlZZ2jrZHL33VVHYWbWGdo2mRx1lHsmZmbN0rbJ5FWvgrvuqjoKM7PO0LbJ\nZMYMePJJePjhqiMxM2t/bZtMRoyA44+Hn/2s6kjMzNpf2yYTgBNOgJ/8pOoozMzaX1snk5NPhu9/\nH9rguUwzs5bW1slk1qwkkaxeXXUkZmbtra2TiQTvfCd87WtVR2Jm1t7acm6urHvvhWOPTWYRnjCh\nuXGZmbUqz801SIcdBm95C1x6adWRmJm1r9KTiaQ5ktZL2iDpgn7qXC6pR9IqSbMGs28en/oUXH45\nrFnT6BHMzKyeUpOJpBHAFcApwExgvqQja+qcChweETOABcCVeffN6+CDk2TytrfBxo0Nf52W0t3d\nXXUILcXtsTe3yZ7cHuUqu2dyLNATEZsiYiewFJhXU2cesAQgIlYCkyRNyblvbmeeCRddlDzIuHgx\n7NzZ6JFag/9h7MntsTe3yZ7cHuUqO5lMBR7IrD+YluWpk2ffQfmrv4If/AC++U2YNi1ZX7IEbr8d\ntm718yhmZo0aVXUAfRiyuwv6cvTRcNNNyXDX97+ffD772eRurx07YOJE2Gef5Ofo0TByJIwalfzM\nLqtOlPW2DbQ977Z77kmSoCXcHntzm+zJ7VGuspPJQ8DBmfVpaVltnYP6qDM6x74v0kB/wXN67LHk\nMxz09Hy86hBaittjb26TPbk9ylN2MrkNOELSdGAzcAYwv6bOMuBc4BuSZgPbImKLpK059gUY0nul\nzcxs8EpNJhGxS9JCYAXJ9ZmrImKdpAXJ5lgcEcslzZW0EdgBnFNv3zLjNTOzxrTFE/BmZlattn8C\nfriQdJWkLZJWZ8r2lbRC0j2SfihpUmbbR9MHPddJemum/GhJq9MHPT/X7O8xVCRNk/QjSWslrZF0\nXlreyW0yRtJKSXembXJxWt6xbQLJM2mS7pC0LF3v2PaQdJ+ku9LfkVvTsua0R0T40wIf4ARgFrA6\nU3YJ8H/S5QuAf06XXwncSTJMeQiwkd/3MlcCr0uXlwOnVP3dGmyPA4FZ6fIE4B7gyE5ukzT+8enP\nkcAvSZ7H6vQ2+SBwDbAsXe/Y9gDuBfatKWtKe7hn0iIi4qfA4zXF84CvpstfBd6eLp8OLI2IFyLi\nPqAHOFbSgcDEiLgtrbcks8+wEhGPRMSqdHk7sI7kjr6ObROAiHg6XRxD8kcg6OA2kTQNmAt8KVPc\nse1B8mhF7d/1prSHk0lrOyAitkDyxxU4IC2vfaDzIX7/oOeDmfLCD3q2AkmHkPTafglM6eQ2SYd0\n7gQeAW5M/8F3cpt8FvgISVLt1cntEcCNkm6T9JdpWVPaoxUfWrT+ddzdEpImANcB50fEdkm1bdBR\nbRIRu4HXSNoHuF7STPZug45oE0mnAVsiYpWkrjpVO6I9UsdHxGZJ+wMrJN1Dk34/3DNpbVvSecpI\nu56PpuX9PejZX/mwJGkUSSK5OiK+mxZ3dJv0iogngW5gDp3bJscDp0u6F7gWeLOkq4FHOrQ9iIjN\n6c//Ar5Dck2tKb8fTiatRew5ncwy4D3p8tnAdzPlZ0gaLelQ4Ajg1rQL+4SkYyUJOCuzz3D0ZeDX\nEXFZpqxj20TSfr134kgaB5xMci2pI9skIi6KiIMj4jCSh5p/FBHvBv6TDmwPSePTnjyS/gB4K7CG\nZv1+VH33gT8v3nHxdeBh4DngfpKHN/cFbiK5k2kFMDlT/6Mkd1+sA96aKX9t+gvUA1xW9fcq0B7H\nA7uAVSR3nNxB8r/wl3Zwm7wqbYdVwGrgY2l5x7ZJ5vucyO/v5urI9gAOzfx7WQNc2Mz28EOLZmZW\nmIe5zMysMCcTMzMrzMnEzMwKczIxM7PCnEzMzKwwJxMzMyvMycQsQ9JT6c/pkvp8s2eBY3+0Zv2n\nQ3l8syo5mZjtqffBq0OBMwezo6SRA1S5aI8TRZwwmOObtTInE7O+fRI4IX3p0vnpbL2fSl9OtUrS\n+wAknSjpx5K+C6xNy65PZ21d0ztzq6RPAuPS412dlj3VezJJ/5LWv0vSn2WOfbOkb6UvL7q6yW1g\nlptnDTbr24XAhyLidIA0eWyLiOMkjQZ+JmlFWvc1wMyIuD9dPycitkkaC9wm6dsR8VFJ50bE0Zlz\nRHrsdwCvjohXSTog3eeWtM4skpcYPZKe8w0R8fMyv7hZI9wzMcvnrcBZ6btEVpLMdzQj3XZrJpEA\nfEDSKpL3r0zL1OvP8SSz3hIRj5LMBvy6zLE3RzLv0SqSN+KZtRz3TMzyEfD+iLhxj0LpRGBHzfqb\ngeMi4jlJNwNjM8fIe65ez2WWd+F/s9ai3DMx21PvH/KngImZ8h8Cf5O+YwVJMySN72P/ScDjaSI5\nEpid2fZ87/415/oJ8OfpdZn9gTcCtw7BdzFrGv8vx2xPvXdzrQZ2p8Na/x4Rl6WvD74jfcfDo/T9\nXuwbgL+WtJZkyu9fZLYtBlZLuj2S924EQERcL2k2cBewG/hIRDwq6Y/7ic2s5XgKejMzK8zDXGZm\nVpiTiZmZFeZkYmZmhTmZmJlZYU4mZmZWmJOJmZkV5mRiZmaFOZmYmVlh/x+zvAdnxm+kxwAAAABJ\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112aae1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Randomly generate our betas and number of observations, used to generate \n",
    "# fake data to fit. We should have a minimum of 4 obs, since we have \n",
    "# four coefficients. \n",
    "true_betas_array = np.random.randint(2, 10, size=4)\n",
    "n_obs = np.random.randint(9500, 10500) \n",
    "\n",
    "# Generate the data that follows a linear relationship specified \n",
    "# by `true_betas_array`. \n",
    "xs, ys = gen_multiple_linear(true_betas_array, n_obs)\n",
    "\n",
    "# Generate the keras model.\n",
    "linear_model = get_keras_model()\n",
    "\n",
    "# Learn the coefficients (perform iterations of forward and backward propagation)\n",
    "linear_model.fit(xs, ys, nb_epoch=5000, verbose=0, batch_size=n_obs)\n",
    "# The history attribute holds a history dictionary that we can use to access the\n",
    "# loss (mean squared error) after each iteration. \n",
    "mean_squared_errors = linear_model.history.history['loss']\n",
    "# Skip the first 100 values because it pulls the y-axis up quite a bit. \n",
    "plot_errors(mean_squared_errors, iterations=(100, 5000))\n",
    "print(\"Final Error: {}\".format(mean_squared_errors[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as with simple linear regression, running our `keras` model is fairly straightforward. We simply call `fit` on it, making sure to pass in our inputs and outputs (`xs` and `ys`) and specify how many iterations of forward and backward propagation to perform over our dataset (this is the `nb_epoch` argument). We also specify the `batch_size` to control how many observations the model looks at in each individual foward/backward propagation step (right now we want it to just look at all of them). We'll detail these parameters more later as we dive into neural networks.\n",
    "\n",
    "Upon running it, we can see that we are also able to solve our multiple linear regression problem using `keras`, and that we obtain the expected coefficients. Next, we'll prove to ourselves that we can also solve multiple logistic regression using this same gradient descent procedure. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
