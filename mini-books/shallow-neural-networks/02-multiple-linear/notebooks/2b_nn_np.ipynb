{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression using Gradient Descent\n",
    "\n",
    "In the last notebook, we walked through a high level overview of using gradient descent to solve our multiple linear regression problem. After building in a little more detail and calculating the derivatives that we'll need to perform the update steps, we'll code it up in `numpy`. \n",
    "\n",
    "## Gradient Descent for Multiple Linear Regression\n",
    "\n",
    "### Gradient Descent Procedure \n",
    "\n",
    "With gradient descent, we'll do the following: \n",
    "\n",
    "1. Randomly initialize values for our coefficients: \n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/variables/beta0.png?raw=true\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=20 \\>, \n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/variables/beta1.png?raw=true\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=22 \\> , \n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/variables/beta2.png?raw=true\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=19 \\> , and\n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/variables/beta3.png?raw=true\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=18 \\>. \n",
    "\n",
    "2. While we haven't met some stopping condition:   \n",
    " A. Calculate our predicted values, \n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/variables/yhat.png?raw=true\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=13 \\>.  \n",
    " B. Calculate the error for each observation using the true values\n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/variables/yi.png?raw=true\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=17 \\>, \n",
    "our predicted values \n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/variables/yhati.png?raw=true\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=16 \\>, \n",
    "and our error formula: \n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/equations/ind_squared_error.png?raw=true\" width=110 \\>      \n",
    " C. For each observation, calculate the gradient of the error with respect to each one of our coefficients (\n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/derivatives/ei_beta0.png?raw=true\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=30\\>, \n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/derivatives/ei_beta1.png?raw=true\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=30\\>, \n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/derivatives/ei_beta2.png?raw=true\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=30\\>, \n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/derivatives/ei_beta3.png?raw=true\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=30\\>\n",
    "), and then use the average across observations to update the coefficients (\n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/variables/alpha.png?raw=true\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=15\\>\n",
    " is the learning rate): \n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/updates/beta0_simp_linear_update.png?raw=true\" width=150 \\>\n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/updates/beta1_simp_linear_update.png?raw=true\" width=150 \\>\n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/updates/beta2_simp_linear_update.png?raw=true\" width=150 \\>\n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/updates/beta3_simp_linear_update.png?raw=true\" width=150 \\>\n",
    "\n",
    "### Derivative Calculations\n",
    "\n",
    "To calculate the gradients for each observation in 2C, we'll use the chain rule that we looked at last notebook: \n",
    "\n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/derivatives/ei_beta0_chain.png?raw=true\" width=120\\>\n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/derivatives/ei_beta1_chain.png?raw=true\" width=120\\>\n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/derivatives/ei_beta2_chain.png?raw=true\" width=120\\>\n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/derivatives/ei_beta3_chain.png?raw=true\" width=120\\>\n",
    "\n",
    "We can break these equations down into calculating each of the individual pieces - \n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/derivatives/ei_yi.png?raw=true\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0\" width=30\\>, \n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/derivatives/yhati_beta0.png?raw=true\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0\" width=30\\>, \n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/derivatives/yhati_beta1.png?raw=true\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0\" width=30\\>\n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/derivatives/yhati_beta2.png?raw=true\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0\" width=30\\>, \n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/derivatives/yhati_beta3.png?raw=true\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0\" width=35\\>. We can calculate those as follows: \n",
    "\n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/derivatives/ei_yi_soln.png?raw=true\" width=275 \\>\n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/derivatives/yhati_beta0_soln.png?raw=true\" width=75 \\>\n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/derivatives/yhati_beta1_soln.png?raw=true\" width=90 \\>\n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/derivatives/yhati_beta2_soln.png?raw=true\" width=90 \\>\n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/derivatives/yhati_beta3_soln.png?raw=true\" width=90 \\>\n",
    "\n",
    "If we plug these back into the original equations, we can obtain our full updates for step 2C: \n",
    "\n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/derivatives/ei_beta0_chain_soln.png?raw=true\" width=350\\>\n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/derivatives/ei_beta1_chain_soln.png?raw=true\" width=290\\>\n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/derivatives/ei_beta2_chain_soln.png?raw=true\" width=290\\>\n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/derivatives/ei_beta3_chain_soln.png?raw=true\" width=290\\>\n",
    "\n",
    "Now, let's code this up! \n",
    "\n",
    "## Multiple Linear Regression using Gradient Descent with `numpy`\n",
    "\n",
    "To demonstrate using gradient descent to solve our multiple linear regression problem, we'll use the `gen_multiple_linear` function from the `datasets/general.py` script to generate some toy data that follows a multivariate linear relationship with three variables. We'll input a `1d numpy array` of betas as well as a number of observations, and it will output data that follows a multivariate linear relationship (\n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/equations/mult_linear_3_feats.png?raw=true\" width=120 style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" \\> ). \n",
    "With this data, we'll use gradient descent to learn the values for our coefficients. \n",
    "\n",
    "In solving our multiple linear regression problem, we'll work exclusively with vectors and matrices. Instead of having individual beta coefficients (like we did with `beta_0` and `beta_1` in simple linear regression), we'll have a beta vector that will hold each of our betas. This means that the first column of the `xs` matrix returned from `gen_multiple_linear` will be a vector of 1's that will be lined up with our \n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/variables/beta0.png?raw=true\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=20 \\>. Aside from this, our solution for multiple linear regression will look largely the same as our solution for simple linear regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets.general import gen_multiple_linear\n",
    "from utils.plotting import plot_errors\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def learn_w_gradient_descent(xs, ys): \n",
    "    learning_rate = 0.1\n",
    "    # Step 1 - randomly initialize values for our coefficients.  \n",
    "    betas_array = np.random.random(size=4)\n",
    "        \n",
    "    mses = [] # mean-squared-errors list to keep track of errors over iterations\n",
    "    for _ in range(5000): \n",
    "        # Step 2A - calculate our predicted values. \n",
    "        yhats = xs.dot(betas_array)\n",
    "        yhats = yhats.reshape(-1, 1) # Force `yhats` to have a second \n",
    "                                     # dimension for later calculations.\n",
    "        \n",
    "        # Step 2B - calculate our errors. \n",
    "        diffs = (ys - yhats)\n",
    "        es = 0.5 * (diffs ** 2)\n",
    "        mses.append(es.mean())\n",
    "        \n",
    "        # Step 2C - calculate the gradient of the error with respect to the coefficients, \n",
    "        # and use it to update the coefficients. \n",
    "        d_betas_array = -diffs * xs\n",
    "        betas_array -= learning_rate * d_betas_array.mean(axis=0)\n",
    "        \n",
    "    return mses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Error: 6.619898311937725e-21\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEZCAYAAACEkhK6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAH95JREFUeJzt3XmUHWWd//H3J4EkJIQ9iyYk7CAIEtCIgEODAgGVOOMc\nJTjCgDqMMyg6yo9FB6IzozLjiojKiI6AEhcGiOfHEvxJs8gWISERsrElZEUwAZJACJ3v74+qJpWb\n7nurm65bfe/9vM65p6ue+9yq731O0t9+lqpSRGBmZlbLgLIDMDOzxuCEYWZmuThhmJlZLk4YZmaW\nixOGmZnl4oRhZma5OGGYNShJN0v6WNlxWOtwwrDSSHpa0npJL0p6Kf15WdlxZUn6e0lzJK2TtFzS\nFZJ2rMN5j860yVpJmyraaWxEnBwR1xQdi1kn+cI9K4ukp4CzIuKOHHUHRkRHrbKeHqNG/c8DXwBO\nB34PjAF+AIwAjoyI1/Ie643EJmk88CSwTfg/rJXIPQwrm7oslM6QdI+kb0l6DrikmzJJ+lLaW1kp\n6X8k7ZAeY3z6l/lZkhYD/0/SYEnXSnpO0mpJD0ga0cX5hwNTgXMi4vaI6IiIJcCHgT2Av5P0prSH\ntFPmcxMk/VnSwHT/LEmPSXpe0i2SxmXqbpL0T5IWAgt72laS7pB0VhfttVrS45LelZYvSdvm9Mxn\nB0n6hqTFklakPafBOWKwFuaEYf3ZO4HHgZHAf3RTdiZJD+AYYC9gOHB5xXH+CtgfOBE4I60zBtgF\n+Efg5S7OfSQwGLghWxgR64CbgeMjYgVwL/ChTJUpwK8jokPSZOAC4IMkvZK7gesqzjMZeAdwYNWW\nyGciMJvke10HTAPeDuwNfAy4XNLQtO6lwD7AIenPMcDFfRCDNTEnDCvbjZL+kv5V/BdJH8+8tywi\nroiITRGxoZuy04BvRcTiiFgPXAicKqnz33YAl0TEK2n9jcCuwH6RmBURa7uIazfguYjY1MV7K9L3\nIfnFfFrmvVOBn6fbZwNfi4iF6XG+DhwqafdM/a9GxAuZ7/dGPBURV6fDVr8ExgJfjoiNEXE78CpJ\ncgD4JPC59Nzr0tim9EEM1sS2KTsAa3mTq8xhPJOj7M3A4sz+YpJ/16MyZUsz21eT/CKdlk5eXwt8\nsYv5g+eA3SQN6CJpvCl9H+B64DJJo4ADgI6I+EP63njgu5K+me6LJIGNyXyPbGxv1KrM9ssAEfFc\nRdn26RDcUOAh6fVRrgF0Mzxo1sk9DCtbtV9SXU3wVpYtJ/nF3Gk8SS8i+8vz9c+kcxH/FhEHkQw7\nfYBkSKvSfcAG4G+2CFbaHjgJ+F16vDXADJKexRSSYaBOS4CzI2KX9LVzRGwfEffX+I5Few5YDxyU\niW2niCh89Zc1NicMa3TXAZ+TtEf6y/w/gGmZXkHlRHGbpLemQ1ZrSZLLVsNOEfEi8BXge5JOlLSN\npD1IhnqWkPRMsjGcTjKX8YtM+Y+AiyQdmJ57R0l/28vv2Zu//rv8TDpk9d/Adzon/CWNkXRCL2Oz\nFuGEYWX7bXpdQefr+h5+/ifANcBdwBMkfzl/JvN+5V/wo4HfAC8AjwJ3pJ/fSkT8F3AR8I20/n0k\nQ17vjYiNmarTgX2BFRExN/P5G0nmBqZJWgPMASZVia2aPL2tWu9n9y8gWTxwfxrbDGC/HsRjLajw\n6zAkTQK+Q5KcroqIS7uo0wZ8G9gW+HNEHFtoUGZm1mOFJoy0278QeA/JWPNM4NSImJ+psyPJ0sQT\nImKZpN0qJurMzKwfKHpIaiKwKF3yuJFkQnByRZ3TgOsjYhlstarDzMz6iaITRnb5ICRLCMdU1NkP\n2CW9anWmfDM1M7N+qT9ch7ENcBhwHDAMuE/SfRHxeLlhmZlZVtEJYxkwLrM/Ni3LWkpyRe0rwCuS\n7gLeRrKC43WSfNM1M7NeiIg+uSiz6CGpmcA+6U3gBpFc3DS9os5NwNGSBqb3uXknMK+rg0VEy70u\nueSS0mPoTy+3h9vDbdKzV18qtIcRyQ3YziFZ4925rHaepLOTt+PKiJgv6TaSNeodwJUR8ViRcZmZ\nWc8VPocREbeS3Ck0W/ajiv1vkFwcZWZm/ZSv9O7n2trayg6hX3F7bMntsTW3SXEa5ol7kqJRYjUz\n6y8kEQ0y6W1mZk3CCcPMzHJxwjAzs1ycMMzMLBcnDDMzy8UJw8zMcnHCMDOzXJwwzMwsFycMMzPL\nxQnDzMxyccIwM7NcnDDMzCwXJwwzM8vFCcPMzHJxwjAzs1ycMMzMLBcnDDMzy8UJw8zMcnHCMDOz\nXJwwzMwsFycMMzPLpaESxqZNZUdgZta6GiphbNhQdgRmZq2roRLGyy+XHYGZWetywjAzs1ycMMzM\nLJfCE4akSZLmS1oo6fwu3j9G0hpJD6evL3V3rFdeKTZWMzPr3jZFHlzSAOBy4D3AcmCmpJsiYn5F\n1bsi4pRax3MPw8ysPEX3MCYCiyJicURsBKYBk7uopzwHc8IwMytP0QljDPBMZn9pWlbpXZJmS/q/\nkg7s7mBOGGZm5Sl0SCqnh4BxEbFe0knAjcB+XVX0HIaZWXmKThjLgHGZ/bFp2esiYm1m+xZJV0ja\nJSL+Unmwa66ZyqxZyXZbWxttbW1FxGxm1rDa29tpb28v5NiKiEIODCBpILCAZNJ7BfAgMCUi5mXq\njIqIVen2ROBXEbFHF8eKn/wkOPPMwsI1M2s6koiIXPPEtRTaw4iIDknnADNI5kuuioh5ks5O3o4r\ngb+V9ClgI/Ay8JHujuchKTOz8hQ+hxERtwL7V5T9KLP9feD7eY7lSW8zs/L4Sm8zM8vFCcPMzHJp\nqIThOQwzs/I0VMJwD8PMrDxOGGZmlosThpmZ5dJQCcNzGGZm5WmohOEehplZeZwwzMwsl4ZKGB6S\nMjMrT0MlDPcwzMzK44RhZma5NFTCWL++7AjMzFpXQyWMdevKjsDMrHU5YZiZWS4NlTAANm4sOwIz\ns9bUUAlj2DD3MszMyuKEYWZmuTRUwhg61AnDzKwsDZUwhg3z0lozs7I0XMJwD8PMrBwNlTA8JGVm\nVp6qCUPSQEl31CuYWjwkZWZWnqoJIyI6gE2SdqxTPFV5SMrMrDzb5KizFpgr6Xbg9V/XEfGZwqLq\nhhOGmVl58iSM/01fpRs61ENSZmZlqZkwIuJnkgYB+6VFCyKilBt0uIdhZlaemglDUhvwM+BpQMDu\nks6IiLuKDW1rw4bB2rX1PquZmUG+IalvAidExAIASfsB1wGHFxlYV4YOhWefrfdZzcwM8l2HsW1n\nsgCIiIXAtnlPIGmSpPmSFko6v0q9d0jaKOlvuqvjISkzs/Lk6WH8UdKPgWvT/Y8Cf8xzcEkDgMuB\n9wDLgZmSboqI+V3U+zpwW7XjOWGYmZUnTw/jU8BjwGfS12NpWR4TgUURsTidKJ8GTO6i3qeB3wBV\nB5y8SsrMrDxVexiSBgI/iYiPAt/qxfHHAM9k9peSJJHsOd4MfDAijpW0xXuV3MMwMytP1YQRER2S\nxksaFBGvFhTDd4Ds3Ia6qzht2lTmz4epU6GtrY22traCQjIza0zt7e20t7cXcmxFRPUK0tXAW4Dp\nbHmld80eh6QjgKkRMSndvyD5aFyaqfNk5yawW3qOf4iI6RXHilmzgjPOgEceyfPVzMxMEhHR7R/i\nPZFn0vuJ9DUAGN7D488E9pE0HlgBnApMyVaIiL06tyX9FPhtZbLo5JsPmpmVJ88cxvCI+EJvDp4O\naZ0DzCBJOFdFxDxJZydvx5WVH6l2PM9hmJmVJ8+Q1H0R8a46xVMtjli9Ohg/Hl54oexozMwaQ72H\npGZLmg78mi3nMOp+Q0IPSZmZlSdPwhgCPA8clykLSriD7bbbggSvvgqDBtX77GZmra3mkFR/ISki\ngp13hiefhJ13LjsiM7P+ry+HpLq90lvSrzLbl1a8N6MvTt4bvmOtmVk5qt0aZN/M9vEV740oIJZc\nhg+Hl14q6+xmZq2rWsKoNlZV2jiWE4aZWTmqTXoPlTSBJKlsl24rfW1Xj+C64oRhZlaOagljBZtv\nOLiSLW8+uLKwiGpwwjAzK0e3CSMijq1nIHk5YZiZlSPP8zD6FScMM7NyOGGYmVkuThhmZpZLt3MY\nkg6r9sGIeLjvw6lt+HB4tuqDXM3MrAjVVkl9M/05BHg78AjJktpDgD8CpdzB1j0MM7NydDskFRHH\npiulVgCHRcTbI+JwYAKwrF4BVnLCMDMrR545jP0jYm7nTkT8ieSRraUYPhxefLGss5uZta48tzef\nI+nHwLXp/keBOcWFVJ17GGZm5ciTMM4EPgWcm+7fBfygsIhqcMIwMytHzYQREa9I+iFwc0QsqENM\nVe2wgxOGmVkZas5hSDoFmA3cmu4fmj6ytRTuYZiZlSPPpPclwERgDUBEzAb2LDKoajoTRoM8KNDM\nrGnkSRgbI+KFirLSfl0PGgQDBsCGDWVFYGbWmvIkjEclnQYMlLSvpO8B9xYcV1UeljIzq788CePT\nwEHABuAXwAvAZ4sMqhYnDDOz+qu6SkrSQOArEfEF4Iv1Cak2Jwwzs/qr2sOIiA7g6DrFkpsThplZ\n/eW5cG9Wuoz218C6zsKI+N/CoqrBCcPMrP7yJIwhwPPAcZmyAEpLGDvsAC9UrtsyM7NC5bnS+8w3\ncgJJk4DvkAx/XRURl1a8fwrwb8AmYCPwuYj4Q7Vj7rSTE4aZWb3VTBiShgAfJ1kpNaSzPCLOyvHZ\nAcDlwHuA5cBMSTdFxPxMtd9FxPS0/sHAr6hxN9yddoI1a2qd3czM+lKeZbXXAKOBE4E7gbFA3hmE\nicCiiFgcERuBacDkbIWIWJ/Z3Z6kp1GVE4aZWf3lSRj7RMS/Ausi4mfA+4B35jz+GOCZzP7StGwL\nkj4oaR7wW6Bmz8UJw8ys/vJMem9Mf66R9FZgJTCyL4OIiBuBGyUdDfw7cHxX9aZOnQrA3LmwenUb\n0NaXYZiZNbz29nba29sLObaixl38JH0CuJ7kWd4/JRk2ujgifljz4NIRwNSImJTuXwBE5cR3xWee\nAN4REX+pKI/OWG++Gb73PbjllloRmJm1NklEhPriWHlWSf043bwT2KuHx58J7CNpPMmzwU8FpmQr\nSNo7Ip5Itw8DBlUmi0oekjIzq788q6Qu7qo8Ir5S67MR0SHpHGAGm5fVzpN0dvJ2XAl8SNLpwKvA\ny8CHax3XCcPMrP7yDEl9PrM7BHg/MC/Pstq+lB2SWr4cDj8cVqyoZwRmZo2nL4ekaiaMLk4+GLgt\nItr6IoAenPf1hLF+Pey6K7z8cj0jMDNrPH2ZMPIsq600lORajNJstx10dMArr5QZhZlZa8kzhzGX\nzU/YGwiMAGrOXxRJgh13TG4PMmRI7fpmZvbG5bkO4/2Z7deAVRHxWkHx5NY58T1qVNmRmJm1hjwJ\no/I2IDtIm4fDai2BLYpvQGhmVl95EsbDwO7AakDATsCS9L2g59dm9AkvrTUzq688k963Ax+IiN0i\nYleSIaoZEbFnRJSSLMAJw8ys3vIkjCMi4ubOnYi4BTiyuJDyccIwM6uvPENSyyV9Cbg23f8oybMt\nSuWEYWZWX3l6GFNIltLekL5GUnE/qDLstBOsXl12FGZmrSPPzQf/ApwLIGlnYE309PLwAuyyCyxZ\nUruemZn1jW57GJIulnRAuj1Y0u+Bx4FVkt5brwC7s+uu8PzzZUdhZtY6qg1JfQRYkG6fkdYdCRwD\nfLXguGpywjAzq69qCePVzNDTicB1EdEREfPIN1leqN12g+eeKzsKM7PWUS1hbJD0VkkjgGNJnmnR\naWixYdXmHoaZWX1V6ymcC/yGZIXUtyPiKQBJJwOz6hBbVbvumvQwIpKbEZqZWbF6/DyMsmSfh9Fp\n2DBYuRKGDy8pKDOzfq7s52H0Gx6WMjOrn4ZOGJ74NjOrn4ZOGO5hmJnVT67lsZKOBPbI1o+IqwuK\nKTf3MMzM6ifPI1qvAfYGZgMdaXEApScM9zDMzOonTw/j7cCB/eH+UZWcMMzM6ifPHMafgNFFB9Ib\nHpIyM6ufPD2M3YDHJD0IbOgsjIhTCosqJ/cwzMzqJ0/CmFp0EL3VebW3mZkVL8/zMO6sRyC9MWIE\n/PnPZUdhZtYaas5hSDpC0kxJayW9KqlD0ov1CK6W0aNh1aqyozAzaw15Jr0vJ3kk6yJgO+ATwPfz\nnkDSJEnzJS2UdH4X758m6ZH0dY+kg/Mee8SIZA6jo6N2XTMze2NyXekdEY8DA9PnYfwUmJTnc5IG\nkCScE4GDgCmdT/HLeBL4q4h4G/DvwH/nDX7bbZNne3sew8yseHkmvddLGgTMlvSfwAry31JkIrAo\nIhYDSJoGTAbmd1aIiPsz9e8HxuQ8NgCjRiXDUqNG9eRTZmbWU3l+8X8srXcOsA7YHfhQzuOPAZ7J\n7C+lekL4BHBLzmMDyTzGypU9+YSZmfVGnlVSiyVtB7wpIr5cVCCSjgXOBI7urs7UqVNf325ra6Ot\nre31HoaZmUF7ezvt7e2FHLvmA5QkfQD4BjAoIvaUdCjwlTwX7kk6ApgaEZPS/QuAiIhLK+odAlwP\nTIqIJ7o5Vpd3J/n855Nexnnn1YrGzKz11PsBSlNJ5iLWAETEbGDPnMefCewjaXw6D3IqMD1bQdI4\nkmTxse6SRTXuYZiZ1UeeSe+NEfGCtnxwdq4bEUZEh6RzgBkkyemqiJgn6ezk7bgS+FdgF+AKJSfZ\nGBET836B0aNhzpy8tc3MrLfyJIxHJZ0GDJS0L/AZ4N68J4iIW4H9K8p+lNn+JPDJvMer5B6GmVl9\n5BmS+jTJNRQbgOuAF4HPFhlUT3iVlJlZfdSc9O4vupv0XrkSDjkEnn22hKDMzPq5vpz07jZhSJre\n5Rupet/evLuE0dEBQ4bA+vXJld9mZrZZXyaManMY7yK56O464AGgT07Y1wYOTOYxVqyAcePKjsbM\nrHlVm8MYDVwEvBX4LnA88FxE3Nnfbnk+diwsXVp2FGZmza3bhJHeaPDWiDgDOAJ4HGhPl8n2K7vv\nDs88U7uemZn1XtVltZIGA+8jub35HsBlwA3Fh9Uz7mGYmRWv24Qh6WqS4aibgS9HxJ/qFlUPjR3r\nHoaZWdGqzWH8HbAvcC5wr6QX09dL/eWJe5123909DDOzonXbw4iIvM+8KJ17GGZmxWuYpFCNexhm\nZsVr+Cu9AV57DYYOhXXrfPGemVlWvW9v3u9tsw2MHJlcvGdmZsVoioQBvhbDzKxoTZMw9twTnnqq\n7CjMzJpX0ySMvfaCJ58sOwozs+blhGFmZrk0VcJ4osdPBDczs7yaJmHsvbd7GGZmRWqK6zAgeZDS\nsGGwejVst10dAzMz68d8HUYXBg6E8ePh6afLjsTMrDk1TcIAz2OYmRWpqRLG3ns7YZiZFaWpEsZ+\n+8HChWVHYWbWnJoqYbzlLfDYY2VHYWbWnJouYcybV3YUZmbNqakSxpgxsH59srTWzMz6VlMlDAkO\nOMC9DDOzIhSeMCRNkjRf0kJJ53fx/v6S7pX0iqR/eaPnO/BAz2OYmRWh22d69wVJA4DLgfcAy4GZ\nkm6KiPmZas8DnwY+2Bfn9DyGmVkxiu5hTAQWRcTiiNgITAMmZytExHMR8RDwWl+c8MAD4dFH++JI\nZmaWVXTCGANkn4O3NC0rzIQJ8PDD0CC3yDIzaxhNNekNyUopgOXLy43DzKzZFDqHASwDxmX2x6Zl\nvTJ16tTXt9va2mhra9uqjgSHHQazZm1OHmZmraK9vZ329vZCjl3o7c0lDQQWkEx6rwAeBKZExFbT\n0pIuAdZGxDe7OVbV25tnXXQRDBkCF1/c69DNzJpCX97evNAeRkR0SDoHmEEy/HVVRMyTdHbydlwp\naRTwR2A4sEnSucCBEbG2t+edMAF+/vO++AZmZtapaR6glPXEE3DssbBkScFBmZn1c36AUg177QXr\n1nni28ysLzVlwpDgqKPgD38oOxIzs+bRlAkD4Oij4Z57yo7CzKx5NG3CePe74e67y47CzKx5NOWk\nN8CGDbDrrsk8xg47FBiYmVk/5knvHAYPhsMPh3vvLTsSM7Pm0LQJA+D44+G228qOwsysOTR1wjjp\nJLjllrKjMDNrDk2dMCZMSB7X+tRTZUdiZtb4mjphDBgAkya5l2Fm1heaOmEAvP/9cOONZUdhZtb4\nmnZZbad165LbnC9cCCNHFhCYmVk/5mW1PTBsGJx8Mlx/fdmRmJk1tqZPGACnngrXXVd2FGZmja3p\nh6QAXn0Vxo2DO++E/ffv48DMzPoxD0n10KBB8PGPww9+UHYkZmaNqyV6GACLFyfP+l6yJJnXMDNr\nBe5h9ML48clT+K68suxIzMwaU8v0MAAeeSS5kO+JJ2Do0D4KzMysH3MPo5fe9jY48ki44oqyIzEz\nazwt1cMAWLAgeRrfnDnwpjf1QWBmZv1YX/YwWi5hAFx0UXJDQl+bYWbNzkNSb9AXvwgPPQS/+EXZ\nkZiZNY6W7GEAzJ6dPGDpnnt8MZ+ZNS/3MPrAoYfCpZcmD1lavrzsaMzM+r9tyg6gTGedBStXwgkn\nJI9yHTOm7IjMzPqvlk4YABdeCAMHJsttp09Plt6amdnWWnZIqpME558PX/86vPe98O1vQ0dH2VGZ\nmfU/hScMSZMkzZe0UNL53dS5TNIiSbMlHVp0TF2ZMgXuvz95Ot+ECcljXRtkPYCZWV0UmjAkDQAu\nB04EDgKmSDqgos5JwN4RsS9wNvDDImOqZu+9ob0dvvxlOO88OPjg5KrwVavKigja29vLO3k/5PbY\nkttja26T4hTdw5gILIqIxRGxEZgGTK6oMxm4GiAiHgB2lDSq4Li6JcFf/zXMnQuXXQZ3350su333\nu+Hii5PJ8dWr6xeP//Fvye2xJbfH1twmxSl60nsM8ExmfylJEqlWZ1laVuLf9UniOO645PXKK3DH\nHUny+OpXk4v+tt8eDjgA9t0X3vxmGD06udXIyJGwww4wfPjm17bblvlNzMz6RsuvkspjyJDkeo2T\nTkr2I2DZsuS+VIsWwYoVyYWAt9wCzz4LL720+fXii0nCGDw4+TloUNc/BwxIkpS05faSJUmyqizv\nqm6Z6nX+hQth5sxyzt2Vstt9wYLkDxjbzG1SnEKv9JZ0BDA1Iial+xcAERGXZur8ELgjIn6Z7s8H\njomIVRXH8hS0mVkv9NWV3kX3MGYC+0gaD6wATgWmVNSZDvwz8Ms0waypTBbQd1/YzMx6p9CEEREd\nks4BZpBMsF8VEfMknZ28HVdGxM2STpb0OLAOOLPImMzMrHca5uaDZmZWrpa/0rveJF0laZWkOZmy\nnSXNkLRA0m2Sdsy8d2F6UeM8SSdkyg+TNCe9IPI79f4efUXSWEm/l/SopLmSPpOWt2SbSBos6QFJ\ns9L2uCQtb8n26CRpgKSHJU1P91u9PZ6W9Ej67+TBtKz4NokIv+r4Ao4GDgXmZMouBf5Pun0+8PV0\n+0BgFsnQ4R7A42zuFT4AvCPdvhk4sezv1sv2GA0cmm5vDywADmjxNhma/hwI3E+yFL1l2yON/3PA\ntcD0dL/V2+NJYOeKssLbxD2MOouIe4DKS/8mAz9Lt38GfDDdPgWYFhGvRcTTwCJgoqTRwPCI6Fxg\nenXmMw0lIlZGxOx0ey0wDxhLa7fJ+nRzMMl/8qCF20PSWOBk4MeZ4pZtj5TYeoSo8DZxwugfRka6\nMiwiVgIj0/LuLmocQ3IRZKelaVlDk7QHSe/rfmBUq7ZJOvwyC1gJ3J7+h27Z9gC+DZxHkjg7tXJ7\nQNIWt0uaKekTaVnhbeIL9/qnlluJIGl74DfAuRGxtovrblqmTSJiEzBB0g7ADZIOYuvv3xLtIel9\nwKqImC2prUrVlmiPjKMiYoWkEcAMSQuow78R9zD6h1Wd989Ku4nPpuXLgN0z9camZd2VNyRJ25Ak\ni2si4qa0uKXbBCAiXgTagUm0bnscBZwi6UngOuA4SdcAK1u0PQCIiBXpzz8DN5LMcxX+b8QJoxxK\nX52mA3+fbp8B3JQpP1XSIEl7AvsAD6bdzRckTZQk4PTMZxrRT4DHIuK7mbKWbBNJu3WubpG0HXA8\nybxOS7ZHRFwUEeMiYi+SC39/HxEfA35LC7YHgKShaY8cScOAE4C51OPfSNmz/a32An4BLAc2AEtI\nLlTcGfgdyQqhGcBOmfoXkqxqmAeckCk/PP1Hsgj4btnf6w20x1FABzCbZCXHwyR/Ue/Sim0CHJy2\nwWxgDvDFtLwl26OibY5h8yqplm0PYM/M/5e5wAX1ahNfuGdmZrl4SMrMzHJxwjAzs1ycMMzMLBcn\nDDMzy8UJw8zMcnHCMDOzXJwwrOVIein9OV5S5RMg3+ixL6zYv6cvj29WJicMa0WdFx/tCZzWkw9K\nGlijykVbnCji6J4c36w/c8KwVvY14Oj0wTznpneJ/c/0AUazJX0SQNIxku6SdBPwaFp2Q3qn0Lmd\ndwuV9DVgu/R416RlL3WeTNJ/pfUfkfThzLHvkPTr9OE219S5Dcxy891qrZVdAHw+Ik4BSBPEmoh4\np6RBwB8kzUjrTgAOiogl6f6ZEbFG0hBgpqTrI+JCSf8cEYdlzhHpsT8EHBIRB0samX7mzrTOoSQP\nuVmZnvPIiLi3yC9u1hvuYZhtdgJwevosigdI7s2zb/reg5lkAfBZSbNJnt0xNlOvO0eR3G2ViHiW\n5C6078gce0Uk9+mZTfJUNLN+xz0Ms80EfDoibt+iUDoGWFexfxzwzojYIOkOYEjmGHnP1WlDZrsD\n/7+0fso9DGtFnb+sXwKGZ8pvA/4pfT4HkvaVNLSLz+8IrE6TxQHAEZn3Xu38fMW57gY+ks6TjADe\nDTzYB9/FrG78l4y1os5VUnOATekQ1P9ExHfTx8Q+nD4f4Fm6fsbxrcA/SnqU5FbS92XeuxKYI+mh\nSJ7bEAARcYOkI4BHgE3AeRHxrKS3dBObWb/j25ubmVkuHpIyM7NcnDDMzCwXJwwzM8vFCcPMzHJx\nwjAzs1ycMMzMLBcnDDMzy8UJw8zMcvn/9MF/PNr/+HgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d027908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Randomly generate our betas and number of observations, used to generate \n",
    "# fake data to fit. We should have a minimum of 4 obs, since we have \n",
    "# four coefficients. \n",
    "true_betas_array = np.random.randint(2, 10, size=4)\n",
    "n_obs = np.random.randint(9500, 10500) \n",
    "\n",
    "# Generate the data that follows a linear relationship specified \n",
    "# by our `true_betas_array`. \n",
    "xs, ys = gen_multiple_linear(true_betas_array, n_obs)\n",
    "\n",
    "# Learn the coefficients using gradient descent. \n",
    "mean_squared_errors = learn_w_gradient_descent(xs, ys)\n",
    "# Skip the first 100 values because it pulls the y-axis up quite a bit. \n",
    "plot_errors(mean_squared_errors, iterations=(100, 5000))\n",
    "print(\"Final Error: {}\".format(mean_squared_errors[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as with simple linear regression, we see that we can in fact solve multiple linear regression using gradient descent. If we run it multiple times, we see that we can achieve effectively 0 mean squared error each time. We could also play around with the `learning_rate` to see how that changes the mean-squared-errors, and/or look at different subests of the `mean_squared_errors` list (iterations ~300-1000 are much more interesting to look at). \n",
    "\n",
    "Viewing multiple linear regression as a computational graph and solving it via gradient descent will help set the stage for understanding complicated neural network architectures that we'll look at in subsequent notebooks. Most (if not all) neural networks can be viewed as having a **forward** and **backward** propagation step where we can use some flavor of gradient descent to update and learn our coefficients (often called weights in neural network land).\n",
    "\n",
    "We'll now move on to coding this up using `theano`, a python library that allows us to define computational graphs and benefit from automatic differentiation. Libraries like this will be extremely useful when building incredibly complicated neural networks for which it is difficult and time consuming to derive the update rules by hand. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
