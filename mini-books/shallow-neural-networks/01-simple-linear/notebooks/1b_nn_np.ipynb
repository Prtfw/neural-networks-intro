{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Linear Regression using Gradient Descent\n",
    "\n",
    "In the previous notebook, we looked at a high level overview of solving our simple linear regression problem using gradient descent. Now, we'll formalize that, actually calculate the derivatives that we need to implement it, and use `numpy` to do so. \n",
    "\n",
    "## Using Gradient Descent for Simple Linear Regression\n",
    "\n",
    "### Gradient Descent Procedure \n",
    "\n",
    "Formally, with gradient descent we will do the following: \n",
    "\n",
    "1. Randomly initialize values for our coefficients, \n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/variables/beta0.png?raw=true\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=20 \\> and\n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/variables/beta1.png?raw=true\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=22 \\>   \n",
    "\n",
    "2. While we haven't met some stopping condition:   \n",
    " A. Calculate our predicted values, \n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/variables/yhat.png?raw=true\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=13 \\>, using our simple linear regression equation\n",
    "(<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/equations/simp_linear.png?raw=true\" width=100 style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" \\>).  \n",
    " B. Calculate the error for each observation using the true values\n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/variables/yi.png?raw=true\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=17 \\>, \n",
    "our predicted values \n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/variables/yhati.png?raw=true\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=16 \\>, \n",
    "and our error formula: \n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/equations/ind_squared_error.png?raw=true\" width=110 \\>      \n",
    " C. For each observation, calculate the gradient of the error with respect to each one of our coefficients (\n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/derivatives/ei_beta0.png?raw=true\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=30\\>, \n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/derivatives/ei_beta1.png?raw=true\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=30\\>\n",
    "), and then use the average across observations to update the coefficients: \n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/updates/beta0_simp_linear_update.png?raw=true\" width=150 \\>\n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/updates/beta1_simp_linear_update.png?raw=true\" width=150 \\>\n",
    "\n",
    "Note that we are subtracting off the gradient in step 2C because the gradient gives us the direction of steepest ascent (and we want to take the direction of steepest descent to minimize our error). Note also that \n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/variables/alpha.png?raw=true\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=15\\> \n",
    "in step 2C is simply the learning rate (e.g. how much of the coefficient updates actually get applied). \n",
    "\n",
    "Now, let's actually calculate the derivatives. \n",
    "\n",
    "### Derivative Calculations\n",
    "\n",
    "Recall that we'll use the chain rule to calculate the updates that we need for step 2C: \n",
    "\n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/derivatives/ei_beta0_chain.png?raw=true\" width=120\\>\n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/derivatives/ei_beta1_chain.png?raw=true\" width=110\\>\n",
    "\n",
    "To calculate these, we'll need three quantities - \n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/derivatives/ei_yi.png?raw=true\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0\" width=30\\>, \n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/derivatives/yhati_beta0.png?raw=true\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0\" width=30\\>, \n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/derivatives/yhati_beta1.png?raw=true\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0\" width=30\\>. We can calculate these as follows: \n",
    "\n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/derivatives/ei_yi_soln.png?raw=true\" width=275 \\>\n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/derivatives/yhati_beta0_soln.png?raw=true\" width=75 \\>\n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/derivatives/yhati_beta1_soln.png?raw=true\" width=90 \\>\n",
    "\n",
    "We can then plug these in to obtain our updates for step 2C: \n",
    "\n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/derivatives/ei_beta0_chain_soln.png?raw=true\" width=350\\>\n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/derivatives/ei_beta1_chain_soln.png?raw=true\" width=290\\>\n",
    "\n",
    "Check out these [notes from Andrej Karpathy](http://karpathy.github.io/neuralnets/) for a refresher on gradient descent or a more thorough but still simplistic discussion of backpropagation (the code in the notes is written in `JavaScript`, but it is fairly simplistic and Andrej does an excellent job with his explanations). \n",
    "\n",
    "## Simple Linear Regression using Gradient Descent with `numpy`\n",
    "\n",
    "We'll begin our `numpy` implementation by generating some fake data. To obtain some fake data that follows a univariate linear relationship, we'll use a function from the `datasets/general.py`. With the function `gen_simple_linear`, we'll input a <img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/variables/beta0.png?raw=true\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=20 \\>, \n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/variables/beta1.png?raw=true\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=22 \\>, \n",
    "and number of observations. We'll receive as output data that follows a univariate linear relationship specified by \n",
    " <img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/variables/beta0.png?raw=true\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=20 \\> \n",
    "and \n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/variables/beta1.png?raw=true\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=22 \\> \n",
    "(<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/equations/simp_linear.png?raw=true\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=110 \\>). Using this data, we'll learn the coefficients using gradient descent as specified above. We'll plot the mean-squared-error over each iteration so that we can see our model learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets.general import gen_simple_linear\n",
    "from utils.plotting import plot_errors\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def learn_w_gradient_descent(xs, ys): \n",
    "    learning_rate = 0.1\n",
    "    # Step 1 - randomly initialize values for our coefficients. \n",
    "    beta_0, beta_1 = np.random.randint(1, 10, size=2)\n",
    "    \n",
    "    mses = [] # mean-squared-errors list to keep track of errors over iterations\n",
    "    for _ in range(5000): \n",
    "        # Step 2A - calculate our predicted values. \n",
    "        yhats = beta_0 + beta_1 * xs\n",
    "        \n",
    "        # Step 2B - calculate our errors. \n",
    "        diffs = (ys - yhats)\n",
    "        es = 0.5 * (diffs ** 2)\n",
    "        mses.append(es.mean())\n",
    "        \n",
    "        # Step 2C - calculate the gradient of the error with respect to the coefficients, \n",
    "        # and use it to update the coefficients.\n",
    "        d_beta_0 = -diffs\n",
    "        d_beta_1 = -diffs * xs\n",
    "        beta_0 -= learning_rate * d_beta_0.mean()\n",
    "        beta_1 -= learning_rate * d_beta_1.mean()\n",
    "    \n",
    "    return mses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Error: 1.2095007887784031e-28\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZkAAAEZCAYAAABFFVgWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a786860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Randomly generate a beta_0, beta_1, and number of observations, used to generate \n",
    "# fake data to fit. We should have a minimum of 2 obs, since we have two coefficients.\n",
    "true_beta_0, true_beta_1 = np.random.randint(2, 10, size=2) \n",
    "n_obs = np.random.randint(9500, 10500) \n",
    "\n",
    "# Generate the data that follows a univariate linear relationship specified \n",
    "# by true_beta_0 and true_beta_1.\n",
    "xs, ys = gen_simple_linear(true_beta_0, true_beta_1, n_obs)\n",
    "\n",
    "# Learn the coefficients using gradient descent. \n",
    "mean_squared_errors = learn_w_gradient_descent(xs, ys)\n",
    "# Skip the first 100 values because it pulls the y-axis up quite a bit. \n",
    "plot_errors(mean_squared_errors, iterations=(100, 5000)) \n",
    "print(\"Final Error: {}\".format(mean_squared_errors[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above demonstrates that we can in fact solve linear regression using gradient descent, as we're able to achieve effectively 0 mean-squared error. We can even run it multiple times, and see that each time, our gradient descent procedure is able to achieve this same 0 error. We could also play around with the `learning_rate` to see how that changes the mean-squared-errors, and/or look at different subests of the `mean_squared_errors` list (iterations ~300-1000 are much more interesting to look at). \n",
    "\n",
    "All in all, this isn't terribly useful. But, it'll help set the stage for understanding complicated neural network architectures. At the end of the day, most (if not all) neural networks can be viewed as having a **forward** and **backward** propagation step where we can use some flavor of gradient descent to update and learn our coefficients (often called weights in neural network land).\n",
    "\n",
    "We'll now move on to coding this up using `theano`, a python library that allows us to define computational graphs and benefit from automatic differentiation. Libraries like this will be extremely useful when building incredibly complicated neural networks for which it is difficult and time consuming to derive the update rules by hand. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
