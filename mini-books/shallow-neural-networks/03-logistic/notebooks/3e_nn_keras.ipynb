{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression using `keras`\n",
    "\n",
    "The [Keras Documentation](http://keras.io/) states that \"Keras is a minimalist, highly modular neural networks library, written in Python and capable of running on top of either TensorFlow or Theano\". In that sense, it's not really meant to be used to perform logistic regression. To continue gradually building towards neural networks, though, we'll walk through it using `keras`.\n",
    "\n",
    "## Computational Graphs for Logistic Regression \n",
    "\n",
    "As a reference, the computational graphs that we used to visualize the forward and backward propagation steps in solving our logistic regression problem with gradient descent are as follows: \n",
    "\n",
    "### Forward Propagation\n",
    "\n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/custom/logistic_comp_graph_condensed_forprop.png?raw=true\" width=300\\>\n",
    "\n",
    "### Backward Propagation\n",
    "\n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/custom/logistic_comp_graph_condensed_backprop.png?raw=true\" width=400\\>\n",
    "\n",
    "### Performing Multiple Linear Regression with Keras\n",
    "\n",
    "Since `keras` can be run on top of either `tensorflow` or `theano`, this means that under the hood of our logistic regression using `keras`, a similar version of the code that we wrote in our `theano` or `tensorflow` implementation is being run. By default, `keras` runs on `theano`, but by [adjusting our keras configuration file](http://keras.io/backend/#switching-from-one-backend-to-another), we can easily change that. For now, though, we'll just run it on `theano`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Input\n",
    "from keras.layers.core import Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from datasets.general import gen_multiple_logistic\n",
    "from utils.plotting import plot_errors\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_keras_model(): \n",
    "    learning_rate = 0.1\n",
    "    # 1. Specify a placeholder for the inputs. \n",
    "    xs = Input(shape=(4,))\n",
    "    # 2. Define the equation that generates predictions. \n",
    "    ys = Dense(1, activation='sigmoid', bias=False)(xs)\n",
    "\n",
    "    # 3. Define a `Model` object that will be used to train/learn the coefficients. \n",
    "    logistic_model = Model(input=xs, output=ys)\n",
    "    \n",
    "    # 4. Define the optimizer and loss function used to train/learn the coefficients. \n",
    "    sgd = SGD(learning_rate)\n",
    "    \n",
    "    # 5. Compile the model (basically, build up the backpropagation steps)\n",
    "    logistic_model.compile(loss='binary_crossentropy', optimizer=sgd)\n",
    "    \n",
    "    return logistic_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the comparisons of multiple linear regression and logistic regression using `theano` and `tensorflow`, we'll see only minor differences between our `keras` implementations for multiple linear regression and logistic regression. Our `get_keras_model` function still returns back what our `get_theano_graph` ([notebook 3c](https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/03-logistic/3c_nn_th.ipynb)) and `get_tensorflow_graph` ([notebook 3d](https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/03-logistic/3d_nn_tf.ipynb)) functions returned - a set of computations that perform forward and backward propagation in order to solve a logistic regression problem using gradient descent. \n",
    "\n",
    "Compared to our `theano` and `tensorflow` implementations, our `get_keras_model` has a smaller code base, which makes sense given it's goal to be a \"minimalist, highly modular neural networks library\". In particular, we see that our forward propagation is defined in 2 steps, compared to the 5 steps it took with `theano` or `tensorflow`: \n",
    "\n",
    "* Step `1` is simply the `keras` way of generating a placeholder variable that will later be replaced with real data. The one piece of information we have to provide is the dimensionality of one of our input observations (e.g. how many features it has). Since we specified that we would have three features and added a column of ones into our `xs` to account for the intercept (\n",
    "<img src=\"../imgs/variables/beta0.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=20 \\>\n",
    "), our `shape` needs to have 4 dimensions.  \n",
    "* Step `2` defines our logistic regression equation, <img src=\"../imgs/equations/logistic.png\" width=125 style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" \\>. To specify logistic regression in place of linear regression, all we have to do is change the **activation** to 'sigmoid'. The use of this activation here will apply the sigmoid function to the sum of the inputs, which is just \n",
    "<img src=\"../imgs/variables/x_beta.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=30 \\> . In later notebooks, we'll detail exactly what that `1` passed into `Dense` refers to, but for now trust that by changing the activation to 'sigmoid', we are performing logistic regression. \n",
    "\n",
    "Backward propagation is defined in steps `4-5`: \n",
    "\n",
    "* Step `4` specifies exactly how to perform our gradient descent updates. Here, we'll use what we've used in all of our prior implementations - gradient descent. As we'll see in later notebooks, there are a number of more complicated flavors of gradient descent that we also have the option to use.\n",
    "* Step `5` tells `keras` to calculate the update rules for our coefficients, defining each of the steps necessary for doing so. Here, we have to specify a `loss` as well as an `optimizer`. As discussed above, the `optimizer` specifies how to perform our gradient descent updates. The `loss` function specifies how we calculate the error, which for logistic regression means binary crossentropy. After defining both a `loss` and `optimizer`, `keras` has all of the pieces it needs to calculate the update rules for our coefficients, and to add those update steps into the graph that it will later run through. \n",
    "\n",
    "Step `3` builds a model object that we can later use to learn our coefficients. To instantiate it, we have to specify `input` as well as `output`. In order to finish building it for later use, we have to run `compile` on it like we did in step `5`. \n",
    "\n",
    "We'll now run our `keras` model and perform logistic regression..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Error: 0.00019978013006038964\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZYAAAEZCAYAAAC0HgObAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112aa35f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Randomly generate our betas and number of observations, used to generate \n",
    "# fake data to fit. We should have a minimum of 4 obs., since we have \n",
    "# 4 coefficients. \n",
    "true_betas_array = np.random.randint(2, 10, size=4)\n",
    "n_obs = np.random.randint(9500, 10500) \n",
    "\n",
    "# Generate the data that follows a logistic relationship specified \n",
    "# by true_betas_array.\n",
    "xs, ys = gen_multiple_logistic(true_betas_array, n_obs)\n",
    "ys = ys > 0.5\n",
    "\n",
    "# Generate the keras model.\n",
    "logistic_model = get_keras_model()\n",
    "\n",
    "# Learn the coefficients (perform iterations of forward and backward propagation)\n",
    "logistic_model.fit(xs, ys, nb_epoch=50000, verbose=0, batch_size=n_obs)\n",
    "# The history attribute holds a history dictionary that we can use to access the\n",
    "# loss (binary crossentropy) after each iteration. \n",
    "binary_crossentropy_lst = logistic_model.history.history['loss']\n",
    "# Skip the first 100 values because it pulls the y-axis up quite a bit. \n",
    "plot_errors(binary_crossentropy_lst, iterations=(100, 50000))\n",
    "print(\"Final Error: {}\".format(binary_crossentropy_lst[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Just as with our linear regression problems, running our `keras` model is fairly straightforward. We simply call `fit` on it, making sure to pass in our inputs and outputs (`xs` and `ys`) and specify how many iterations of forward and backward propagation to perform over our dataset (this is the `nb_epoch` argument). We also specify the `batch_size` to control how many observations the model looks at in each individual foward/backward propagation step (right now we want it to just look at all of them). We'll detail these parameters more later as we dive into neural networks. \n",
    "\n",
    "Upon running it, we can see that we are also able to solve our multiple linear regression problem using `keras`. Next, we'll actually start coding up our first true neural network! "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
