{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression using `theano`\n",
    "\n",
    "We'll now walk through using gradient descent to solve logistic regression with `theano`. Just as with our linear regression problems, we'll begin by building up a computational graph, and then letting `theano` handle the differentiation for us. \n",
    "\n",
    "With the automatic differentiation that `theano` offers, we'll still have to specify the details of what derivatives to compute, but will not actually have to take any by hand. This feature will become invaluable as we move towards neural networks with hundreds of thousands of parameters. \n",
    "\n",
    "## Computational Graphs for Logistic Regression \n",
    "\n",
    "As we code up our forward and backward propagation steps with `theano`, let's keep their visuals around as a reference: \n",
    "\n",
    "### Forward Propagation\n",
    "\n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/custom/logistic_comp_graph_condensed_forprop.png?raw=true\" width=300\\>\n",
    "\n",
    "### Backward Propagation\n",
    "\n",
    "<img src=\"https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/imgs/custom/logistic_comp_graph_condensed_backprop.png?raw=true\" width=400\\>\n",
    "\n",
    "### Building a computational graph with `theano`\n",
    "\n",
    "As previously mentioned, the biggest difference between our `numpy` and `theano` solutions for logistic regression is that we'll be able to use the automatic differentiation that `theano` offers. In order to do so, we'll have to tell `theano` what quantity to take the derivative of and the parameters that it should take the derivative with respect to, but after that it'll handle the rest. \n",
    "\n",
    "Let's take a look..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "from datasets.general import gen_multiple_logistic\n",
    "from utils.plotting import plot_errors\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_theano_graph(): \n",
    "    learning_rate = 0.1\n",
    "    # 1. Define placeholder matrices for inputs. \n",
    "    xs, ys = T.dmatrices('xs', 'ys') # returns one or more matrices of type `float64`\n",
    "    # 2. Define randomly initialized floats for our betas. \n",
    "    betas = theano.shared(np.random.random(size=(4, 1)), name='betas')\n",
    "\n",
    "    # 3. Define the equation that generates predictions. \n",
    "    yhats = 1 / (1 + T.numpy.exp(-T.dot(xs, betas)))\n",
    "    # 4. Define the equation that generates our errors. \n",
    "    es = -(ys * np.log(yhats) + (1 - ys) * np.log(1 - yhats))\n",
    "    # 5. Define the aggregate cost\n",
    "    E = es.mean()\n",
    "    # 6. Take advantage of `theanos` automatic differentiation, and use the derivatives\n",
    "    #    to perform the update step. \n",
    "    d_betas = T.grad(E, betas)\n",
    "    updates = [(betas, betas - learning_rate * d_betas)]\n",
    "    # 7. Define a function that we can feed inputs to, obtain outputs from, and \n",
    "    #    perform updates on our coefficients / train them. \n",
    "    train = theano.function(inputs=[xs, ys], outputs=[E, yhats], \n",
    "                            updates=updates)\n",
    "    \n",
    "    return train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our solution here for logistic regression is going to look extremely similar to our solution for multiple linear regression ([notebook 2c](https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/02-multiple-linear/2c_nn_th.ipynb)). The only differences will be in the calculations of the predicted values (`yhats`) and errors (`es`) in steps `3` and `4`. Let's walk through each piece of the code to refresh our memory on how it works...\n",
    "\n",
    "From a high level, `get_theano_graph` returns a `theano.function` object that performs one iteration of our gradient descent procedure. It performs both forward propagation (steps `1-5`) and backward propagation (step `6`), and in the end [generates a callable function](http://deeplearning.net/software/theano/library/compile/function.html#module-theano.compile.function) that we can later use to perform one iteration of our gradient descent procedure (step `7`).  \n",
    "\n",
    "In step `6`, we use `T.grad` to take advantage of the automatic differentiation that `theano` offers. With `T.grad`, we pass as the first argument the quantity to take the derivative of and as the second what to take the derivative with respect to. Recall that the `T.grad` function expects a scalar input, which is why we differentiate the **mean** of the individual binary crossentropy calculations. Mathematically, taking the derivative of the mean is the same as taking the mean of the individual derivatives, since the derivative of a sum is equal to the sum of the derivatives. \n",
    "\n",
    "The callable function generated in step `7` takes inputs (via the `inputs` argument), runs them through the graph, and returns outputs (specified by the `outputs` argument). This function has the side effect of updating the values of each of the betas in our `betas` shared variable (specified by the `updates` argument), which is ultimately how we learn the values for our coefficients. \n",
    "\n",
    "It's important to again note that our `betas` variable is defined as a [shared variable](http://deeplearning.net/software/theano/tutorial/examples.html#using-shared-variables), which tells `theano` that it should **share** the values held in this variable across calls to the function generated in step `7`. As a result, `theano` uses the `betas` variable values from the previous call to our function as the `betas` variable values that it updates in the current call. This ensures that we are continuously updating our `betas` through each call to our function, and over time converging to their true values.\n",
    "\n",
    "Now, we'll dive into some code that uses this function to learn the true values for our `betas` vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Error: 0.0066391832793999345\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZYAAAEZCAYAAAC0HgObAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1121940f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Randomly generate our betas and number of observations, used to generate \n",
    "# fake data to fit. We should have a minimum of 4 obs., since we have \n",
    "# 4 coefficients. \n",
    "true_betas_array = np.random.randint(2, 10, size=4)\n",
    "n_obs = np.random.randint(9500, 10500) \n",
    "\n",
    "# Generate the data that follows a logistic relationship specified \n",
    "# by true_betas_array.\n",
    "xs, ys = gen_multiple_logistic(true_betas_array, n_obs)\n",
    "ys = ys > 0.5\n",
    "\n",
    "theano_linear_graph = get_theano_graph()\n",
    "binary_crossentropy_lst = []\n",
    "for _ in range(50000): \n",
    "    binary_crossentropy, yhats = theano_linear_graph(xs, ys)\n",
    "    binary_crossentropy_lst.append(binary_crossentropy)\n",
    "# Skip the first 100 values because it pulls the y-axis up quite a bit. \n",
    "plot_errors(binary_crossentropy_lst, iterations=(100, 50000))\n",
    "print(\"Final Error: {}\".format(binary_crossentropy_lst[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As we can see, this implementation is pretty similar to our implementation using `numpy`. The biggest difference is that we call our function `theano_linear_graph` at each iteration of our loop as opposed to having the entire looping process defined within a function. Given that our `theano_linear_graph` by definition performs only **a single** iteration of our gradient descent procedure, this makes sense. \n",
    "\n",
    "Overall, we see that we can solve our logistic regression problem using this computational graph that we defined with `theano`. Now, we'll code this up using `tensorflow`."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
